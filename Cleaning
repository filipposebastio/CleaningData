##pick specific type of column and apply an operation/transfortmation
#in this case, from object to category
df[df.select_dtypes(['object']).columns] = df.select_dtypes(['object']).apply(lambda x: x.astype('category'))
#select only numerical variables and create new dataset
df_num = df.select_dtypes(include=[np.float])

### Append multiple files together
path =r'C:\Users\filippo.sebastio\Desktop\files' # use your path
allFiles = glob.glob(path + "/*.csv")
frame = pd.DataFrame()
list_ = []
for file_ in allFiles:
    df = pd.read_csv(file_,index_col=None, header=0)
    list_.append(df)
frame = pd.concat(list_)

##Read files
df = pd.read_excel(r'C:\Users\filippo.sebastio\Desktop\files\country_risk\country_risk.xlsx')


##groupby - groupby by country (or group) and return a variable with the count name 'counts'
df = df.groupby('country').size().reset_index(name = 'counts')
#by countyr, get the mean of price and points and the count of countries
df = df.groupby('country').agg({'price':'mean', 'points':'mean', 'country':'count'})

###grouby and resample 
df_sub_rg = df_tr.groupby('ISO_Sub_Region').resample('Y')['Transparency'].mean()
df_sub_rg = df_tr.groupby('ISO_Sub_Region').resample('Y')['Transparency_numeric'].mean().to_frame('count').reset_index()


#logical filtering 
#Be careful, columns can contain both object and numbers
df['var1'] = np.where(df['var2'] > 400, 'high', 
             (np.where(df['var2'] < 200, 'low', Other value not mapped)))

#from string/float to numeric
df['var1'] = pd.to_numeric(df['var1'], errors = 'coerce')

#######pick all columns with numbers and make them numeric from object
#Drop columns names 
cols = df.columns.drop(['Country','Overall Risk Band', 'Geography Risk Band'])
df[cols] = df[cols].apply(pd.to_numeric, errors='coerce')

#Drops columns
df_num = df_num.drop(['Geography Confidence','Confidence Score'], axis=1) 

#rename columns
df = df.rename({"1":'One', '2':"Two"})

#dropping missing values
country = country[np.isfinite(country['wage_gap_%'])]

####APPLY OPERATION TO ONLY SPECIFIC COLUMN
turkey[['Afghanistan', 'Azerbaijan', 'Ethiopia', 'Georgia', 'Iran, Islamic Republic of', 'Russian Federation', 'Syrian Arab Republic', 'Turkey']] = turkey[['Afghanistan', 'Azerbaijan', 'Ethiopia', 'Georgia', 'Iran, Islamic Republic of', 'Russian Federation', 'Syrian Arab Republic', 'Turkey']].fillna(value=0)

##looping through columns 
col_list = df[['Q6', 'Q7','Q8', 'Q9']]
for col in col_list:
    df = df[df[col].isin(list3)]

##Iterate and create new columns using prefix and suffix!
for col in pdi_bei[col]:
    pdi_bei[col+'_to_numeric'] = pd.to_numeric(pdi_bei[col], errors = 'coerce')

##Filtering data on specific values
list= ["Man", "Woman"]
df = df[df['gender'].isin(list)]

pd.get_option("display.max_rows")    

###Working with header
df = df.drop(['Unnamed: 1'], axis=1)
second = df.columns[1]
df["Corso" ] = second

new_header = df.iloc[0] #grab the first row for the header
df = df[2:] #take the data less the header row
df.columns = new_header #set the header row as the df header


####Regular expression
#keep only observations with specific letter (ex 'sia') in objectcolumn
asia = df_ts[df_ts.ISO_Sub_Region.str.contains('sia')]


# import datetime
# df_f_c = df.groupby(['Factory', 'Count']).first().reset_index()
# df_f_c['Initial Date'] =  np.where((df_f_c.Factory.str.contains('Chung Sen')) & (df_f_c['Count']==0) , datetime.datetime(2018, 1, 1), np.NaN)
# df_f_c['Initial Date'] =  np.where((df_f_c.Factory.str.contains('Hansae')) & (df_f_c['Count']==0) , datetime.datetime(2017, 12, 1), np.NaN)
# df_f_c['Initial Date'] =  np.where((df_f_c.Factory.str.contains('Texpia')) & (df_f_c['Count']==0) , datetime.datetime(2018, 1, 1), np.NaN)
# df_f_c['Initial Date'] =  np.where((df_f_c.Factory.str.contains('Jingsheng')) & (df_f_c['Count']==0) , datetime.datetime(2018, 1, 1), np.NaN)
# df_f_c['Initial Date'] =  np.where((df_f_c.Factory.str.contains('Putian')) & (df_f_c['Count']==0) , datetime.datetime(2018, 1, 1), np.NaN)




# df_f_c[['Factory', 'Initial Date']]

