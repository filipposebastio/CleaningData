##Single tab
my_tab = pd.crosstab(index=titanic_train["Survived"],  # Make a crosstab
                              columns="count")      # Name the count column

##cross tab with percentage instead of frequency and margins - percentaged on row
speak = pd.crosstab(df['gender'], df['Q4 speak_up'], margins=True, normalize = 'index')

#Simple OLS 
import statsmodels.api as sm
reg1 = sm.OLS(endog=df['Happiness.Score'], exog=df[['index', 'gdp']], missing='drop')
type(reg1)
results = reg1.fit()
type(results)
print(results.summary())
###Robust standard errors!
results_robust = results.get_robustcov_results()
print(results_robust.summary())

##Run multiple scatterplot with a y-specific column picking only numberic variables as your X
df_num = df.select_dtypes(include = ['float64', 'int64'])
df_num = df_num[df_num['Number of workers']<60000]
df_num = df_num[df_num['Paid above monthly (%)']<101]
for i in range(0, len(df_num.columns), 5):
    sns.pairplot(data=df_num,
                x_vars=df_num.columns[i:i+5],
                y_vars=["Number of findings (total)"])


###EDA by group 
turkey_dummy.groupby('Syrian Arab Republic')['Labour Standards Findings'].describe()


###Text analysis

sub['POS'] = pos_tag_sents(sub['body'].apply(word_tokenize).tolist() )
def extract_pos(line,pos):
    return [word[0] for word in line if word[1] == pos]
sub['NNP'] = [extract_pos(line,'NNP') for line in sub['POS']]
sub['JJ'] = [extract_pos(line,'JJ') for line in sub['POS']]
sub['NN'] = [extract_pos(line,'NNP') for line in sub['POS']]
sub['FW'] = [extract_pos(line,'FW') for line in sub['POS']]



##Sentiment 

df_lower['sentiment'] = df_lower['body'].apply(lambda tweet: TextBlob(tweet).sentiment)
df_lower['Polarity'], df_lower['Subjectivity'] = zip(*df_lower.sentiment)



##number of individual words in pandas columns

import nltk

top_N = 100

txt = dfhs['Good list lower'] .str.lower().str.replace(r'\|', ' ').str.cat(sep=' ')
words = nltk.tokenize.word_tokenize(txt)
word_dist = nltk.FreqDist(words)

stopwords = nltk.corpus.stopwords.words('english')
words_except_stop_dist = nltk.FreqDist(w for w in words if w not in stopwords) 

# print('All frequencies, including STOPWORDS:')
# print('=' * 60)
# rslt = pd.DataFrame(word_dist.most_common(top_N),
#                     columns=['Word', 'Frequency'])
print('=' * 60)

rslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),
                    columns=['Word', 'Frequency']).set_index('Word')
rslt
# plt.style.use('ggplot')

# plt = rslt.plot.bar(rot=0)


